# ğŸ¤– CraCha - Crawl Chat Agent

Ein intelligentes RAG (Retrieval-Augmented Generation) System mit modernster UX und verteilter Cloud-Architektur fÃ¼r die Erstellung von Wissensdatenbanken aus Webinhalten.

## ğŸš€ Live Demo

[Streamlit Community Cloud App](https://your-app-url.streamlit.app) (wird nach Deployment verfÃ¼gbar)

## ğŸ—ï¸ Architektur

- **Frontend**: Streamlit Community Cloud mit modernem UI/UX
- **Crawling**: Modal.com Serverless Service mit Crawl4AI
- **Vektordatenbank**: ChromaDB (In-Memory)
- **Embeddings**: Google Vertex AI Multilingual oder ChromaDB Standard
- **LLM**: Google Gemini 2.5 Flash fÃ¼r RAG-Antworten

## âœ¨ Hauptfeatures

### ğŸŒ Intelligentes Web-Crawling
- **Automatische URL-Typ-Erkennung**: Website, Sitemap, Einzelseite, Dokumentation
- **Real-time URL-Validierung**: Sofortige ÃœberprÃ¼fung von Erreichbarkeit und GÃ¼ltigkeit
- **Optimierte Einstellungen**: Automatische Anpassung basierend auf Website-Typ
- **Flexible Konfiguration**: Crawling-Tiefe, Seitenlimits, Chunk-GrÃ¶ÃŸe anpassbar

### ğŸ“š Wissensdatenbank-Management
- **Intelligente Textaufteilung**: Optimales Chunking fÃ¼r prÃ¤zise Suche
- **Mehrsprachige Embeddings**: UnterstÃ¼tzung fÃ¼r deutsche und internationale Inhalte
- **Automatische Optimierung**: Memory-Management und Performance-Tuning
- **Batch-Verarbeitung**: Effiziente Verarbeitung groÃŸer Datenmengen

### ğŸ’¬ Erweiterte Chat-Funktionen
- **RAG-basierte Antworten**: Kontextuelle Antworten basierend auf gecrawlten Inhalten
- **Chat-Export**: VollstÃ¤ndige GesprÃ¤chsverlÃ¤ufe als Markdown exportieren
- **Session-Management**: Persistente Chat-Historie pro Wissensdatenbank
- **Typing-Indikatoren**: Moderne Chat-UX mit visuellen Feedback-Elementen

### ğŸ¨ Moderne BenutzeroberflÃ¤che
- **Minimalistisches Design**: Fokus auf Benutzerfreundlichkeit
- **Real-time Feedback**: Sofortige Validierung und Status-Updates
- **Responsive Layout**: Optimiert fÃ¼r Desktop und mobile GerÃ¤te
- **Intelligente BenutzerfÃ¼hrung**: Schritt-fÃ¼r-Schritt Anleitung durch den Prozess

## ğŸ› ï¸ Lokale Entwicklung

1. Repository klonen:
```bash
git clone https://github.com/NicoDev01/Crawl_RAG_Agent_streamlit_demo.git
cd Crawl_RAG_Agent_streamlit_demo
```

2. Dependencies installieren:
```bash
pip install -r requirements.txt
```

3. Streamlit App starten:
```bash
streamlit run streamlit_app.py
```

## âš™ï¸ Konfiguration

### Erforderliche Secrets (Streamlit Cloud)

```toml
# Modal.com Crawling Service (ERFORDERLICH)
MODAL_API_URL = "https://nico-gt91--crawl4ai-service"
MODAL_API_KEY = "your-modal-api-key"

# Google Cloud fÃ¼r Embeddings und LLM (ERFORDERLICH)
GOOGLE_CLOUD_PROJECT = "your-gcp-project-id"
GOOGLE_CLOUD_LOCATION = "us-central1"
GOOGLE_APPLICATION_CREDENTIALS_JSON = "base64-encoded-service-account-json"

# Google Gemini API fÃ¼r RAG-Antworten (ERFORDERLICH)
GEMINI_API_KEY = "your-gemini-api-key"
```

### Optionale Secrets (fÃ¼r erweiterte Features)

```toml
# Vertex AI Reranker (optional, fÃ¼r verbesserte Suchergebnisse)
VERTEX_RERANKER_MODEL = "text-reranking-model"

# OpenAI (optional, als Alternative zu Gemini)
OPENAI_API_KEY = "your-openai-api-key"
```

### Lokale Entwicklung (.env Datei)

```bash
# Kopiere .env.example zu .env und fÃ¼lle die Werte aus
cp .env.example .env

# Erforderliche Umgebungsvariablen
MODAL_API_KEY=your-modal-api-key
GOOGLE_CLOUD_PROJECT=your-gcp-project
GEMINI_API_KEY=your-gemini-api-key
```

## ğŸ“– Verwendung

### 1. ğŸ“š Wissensdatenbank erstellen

1. **URL eingeben**: Gib die URL der Website ein, die du crawlen mÃ¶chtest
   - UnterstÃ¼tzt: Websites, Sitemaps, Einzelseiten, Dokumentations-Sites
   - Real-time Validierung prÃ¼ft Erreichbarkeit und GÃ¼ltigkeit

2. **Einstellungen anpassen**: 
   - **Crawling-Tiefe**: Wie tief sollen Links verfolgt werden (Standard: 1)
   - **Seitenlimit**: Maximale Anzahl zu crawlender Seiten (Standard: 1)
   - **Erweiterte Optionen**: Chunk-GrÃ¶ÃŸe, Parallelisierung, Auto-Optimierung

3. **Wissensdatenbank benennen**: Eindeutigen Namen fÃ¼r die Datenbank vergeben

4. **Crawling starten**: Prozess wird mit Live-Progress-Tracking ausgefÃ¼hrt

### 2. ğŸ’¬ Chat mit der Wissensdatenbank

1. **Datenbank auswÃ¤hlen**: WÃ¤hle eine der erstellten Wissensdatenbanken
2. **Fragen stellen**: Stelle natÃ¼rlichsprachliche Fragen zu den Inhalten
3. **Antworten erhalten**: Erhalte kontextuelle Antworten basierend auf den gecrawlten Inhalten
4. **Chat exportieren**: Exportiere GesprÃ¤chsverlÃ¤ufe als Markdown-Datei

### ğŸ’¡ Beispiel-AnwendungsfÃ¤lle

- **Produktdokumentation**: Crawle Dokumentations-Websites fÃ¼r interne Wissensdatenbanken
- **Competitive Intelligence**: Analysiere Konkurrenz-Websites und stelle gezielte Fragen
- **Content Research**: Erstelle durchsuchbare Archive von Fachartikeln und Blogs
- **Support-Systeme**: Baue FAQ-Systeme basierend auf bestehenden Inhalten auf

## ğŸ”§ Technische Details

### Backend-Architektur
- **Crawling Service**: Modal.com Serverless mit Crawl4AI und Playwright
- **Text Processing**: Intelligentes Chunking mit konfigurierbarer GrÃ¶ÃŸe (500-2500 Zeichen)
- **Embeddings**: Google Vertex AI `text-multilingual-embedding-002` fÃ¼r mehrsprachige UnterstÃ¼tzung
- **Vector Search**: ChromaDB mit Cosine Similarity und optionalem Reranking
- **LLM Integration**: Google Gemini 2.5 Flash fÃ¼r RAG-Antwortgenerierung

### Frontend-Technologien
- **UI Framework**: Streamlit mit modernem CSS-Styling
- **Session Management**: Persistente ZustÃ¤nde und Chat-Historie
- **Real-time Validation**: Debounced URL-Validierung mit Caching
- **Progress Tracking**: Live-Updates wÃ¤hrend Crawling-Prozess
- **Export-Funktionen**: Markdown-Export fÃ¼r Chat-VerlÃ¤ufe

### UX-Komponenten
- **URLValidator**: Real-time URL-Validierung mit visuellen Indikatoren
- **ProcessDisplay**: Detailliertes Progress-Tracking mit erweiterbaren Logs
- **SuccessAnimation**: Celebration-Animationen und Erfolgs-Metriken
- **AutoCompleteHandler**: Intelligente NamensvorschlÃ¤ge basierend auf Seitentiteln

### Sicherheit & Performance
- **Input-Validierung**: Umfassende URL- und Parameter-Validierung
- **Memory-Management**: Automatische Optimierung bei groÃŸen Datenmengen
- **Error-Handling**: Robuste Fehlerbehandlung mit benutzerfreundlichen Nachrichten
- **Caching**: Intelligentes Caching fÃ¼r URL-Validierung und Embeddings

## ğŸ“ Lizenz

MIT License

## ğŸ¤ Beitragen

Pull Requests sind willkommen! FÃ¼r grÃ¶ÃŸere Ã„nderungen Ã¶ffne bitte zuerst ein Issue.

## ğŸ†• Neueste Updates

### Version 2.0 - UX Revolution
- **ğŸ¨ Komplett Ã¼berarbeitete BenutzeroberflÃ¤che**: Minimalistisches, modernes Design
- **âš¡ Real-time URL-Validierung**: Sofortige ÃœberprÃ¼fung mit visuellen Indikatoren
- **ğŸ§  Intelligente URL-Typ-Erkennung**: Automatische Optimierung basierend auf Website-Typ
- **ğŸ”„ Verbesserte BenutzerfÃ¼hrung**: Schritt-fÃ¼r-Schritt Anleitung ohne Verwirrung
- **ğŸ’¬ Erweiterte Chat-Funktionen**: Export, Timestamps, bessere UX
- **ğŸ›¡ï¸ Robuste Fehlerbehandlung**: Benutzerfreundliche Fehlermeldungen und Tipps

### Version 1.5 - Performance & StabilitÃ¤t
- **ğŸ“ˆ Optimierte Crawling-Performance**: Bis zu 50% schnellere Verarbeitung
- **ğŸ”§ Modulare UX-Komponenten**: Wiederverwendbare UI-Bausteine
- **ğŸ’¾ Verbessertes Memory-Management**: Automatische Optimierung bei groÃŸen Datenmengen
- **ğŸŒ Mehrsprachige UnterstÃ¼tzung**: Optimiert fÃ¼r deutsche und internationale Inhalte

## ğŸ—ºï¸ Roadmap

### Geplante Features
- [ ] **Batch-Upload**: Mehrere URLs gleichzeitig verarbeiten
- [ ] **Advanced Analytics**: Detaillierte Statistiken Ã¼ber Crawling-Ergebnisse
- [ ] **Custom Embeddings**: Support fÃ¼r weitere Embedding-Modelle
- [ ] **API-Zugang**: REST API fÃ¼r programmatischen Zugriff
- [ ] **Collaborative Features**: Team-basierte Wissensdatenbanken

## ğŸ“ Support

Bei Fragen oder Problemen:
- ğŸ› **Bugs**: Ã–ffne ein [GitHub Issue](https://github.com/NicoDev01/Crawl_RAG_Agent_streamlit_demo/issues)
- ğŸ’¡ **Feature Requests**: Diskutiere in [GitHub Discussions](https://github.com/NicoDev01/Crawl_RAG_Agent_streamlit_demo/discussions)
- ğŸ“§ **Direkte Anfragen**: Kontaktiere den Entwickler Ã¼ber GitHub